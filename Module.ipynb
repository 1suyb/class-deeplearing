{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "eps = 1e-8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculation Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiplication Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mul :\n",
    "    def __init__(self, W) :\n",
    "        self.W = W\n",
    "    \n",
    "    def forward(self, X) :\n",
    "        self.X = X\n",
    "        return np.dot(self.X, self.W)\n",
    "\n",
    "    def backward(self,dY) :\n",
    "        X = self.X\n",
    "        W = self.W\n",
    "        dX = np.dot(dY, W.T)\n",
    "        dY = np.dot(X.T, dY)\n",
    "        return (dX,dY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4]]\n",
      "[[2 2]]\n",
      "[[1]\n",
      " [1]]\n"
     ]
    }
   ],
   "source": [
    "X = np.full((1,2),1)\n",
    "W = np.full((2,1),2)\n",
    "\n",
    "model = Mul(W)\n",
    "\n",
    "y = model.forward(X)\n",
    "print(y)\n",
    "dx1,dx2 = model.backward(np.ones_like(y))\n",
    "print(dx1)\n",
    "print(dx2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4.]], grad_fn=<MmBackward0>)\n",
      "tensor([[2., 2.]])\n",
      "tensor([[1.],\n",
      "        [1.]])\n"
     ]
    }
   ],
   "source": [
    "x1 = torch.full((1,2),1.,requires_grad=True)\n",
    "x2 = torch.full((2,1),2.,requires_grad=True)\n",
    "y = torch.matmul(x1,x2)\n",
    "print(y)\n",
    "y.backward(torch.ones_like(y))\n",
    "print(x1.grad)\n",
    "print(x2.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Addition Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Add :\n",
    "    def __init__ (self,B) :\n",
    "        self.B = B\n",
    "    def forward(self,X) :\n",
    "        return X+self.B\n",
    "    def backward(self,dY) :\n",
    "        db = np.sum(dY,axis=0)\n",
    "        return (dY,db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X :\n",
      " [[3 3]\n",
      " [3 3]] \n",
      "b :\n",
      " [1]\n",
      "y :\n",
      " [[4 4]\n",
      " [4 4]]\n",
      "dx :\n",
      " [[1 1]\n",
      " [1 1]]\n",
      "db :\n",
      " [2 2]\n"
     ]
    }
   ],
   "source": [
    "X = np.full((2,2),3)\n",
    "b = np.full((1,),1)\n",
    "print(\"X :\\n\",X,\"\\nb :\\n\",b)\n",
    "\n",
    "model = Add(b)\n",
    "\n",
    "y = model.forward(X)\n",
    "print(\"y :\\n\",y)\n",
    "dX,db = model.backward(np.ones_like(y))\n",
    "print(\"dx :\\n\", dX)\n",
    "print(\"db :\\n\", db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X :  tensor([[3., 3.],\n",
      "        [3., 3.]], requires_grad=True) \n",
      "b :  tensor([1.], requires_grad=True)\n",
      "y : \n",
      " tensor([[4., 4.],\n",
      "        [4., 4.]], grad_fn=<AddBackward0>)\n",
      "dx :\n",
      " tensor([[1., 1.],\n",
      "        [1., 1.]])\n",
      "db :\n",
      " tensor([4.])\n"
     ]
    }
   ],
   "source": [
    "X = torch.full((2,2),3.,requires_grad=True)\n",
    "b = torch.full((1,),1.,requires_grad=True)\n",
    "print(\"X : \",X,\"\\nb : \",b )\n",
    "y = X+b\n",
    "print(\"y : \\n\",y)\n",
    "y.backward(torch.ones_like(y))\n",
    "print(\"dx :\\n\", X.grad)\n",
    "print(\"db :\\n\", b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Repetition Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Repeat :\n",
    "    def forward(self,X,N,axis) :\n",
    "        self.axis = axis\n",
    "        return np.repeat(X,N,axis=axis)\n",
    "    def backward(self,dy) :\n",
    "        return np.sum(dy, axis=self.axis, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Unsqueeze :\n",
    "    def forward(self, X, axis) :\n",
    "        self.axis = axis\n",
    "        return np.expand_dims(X,axis = axis)\n",
    "    def backward(self,dY) :\n",
    "        return np.squeeze(dY,axis = self.axis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Squeeze :\n",
    "    def forward(self,X,axis) :\n",
    "        self.axis = axis\n",
    "        return np.squeeze(X,axis=axis)\n",
    "    def backward(self,dY) :\n",
    "        return np.expand_dims(dY,axis=self.axis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SumLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sum : \n",
    "    def forward(self,X,axis) :\n",
    "        self.axis = axis\n",
    "        self.N = X.shape[axis]\n",
    "        return np.sum(X,axis=axis,keepdims=True)\n",
    "    \n",
    "    def backward(self,dy) :\n",
    "        return np.repeat(dy,self.N,axis=self.axis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation Function Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU:\n",
    "    def forward(self,X) :\n",
    "        Y = X.copy()\n",
    "        self.mask = (X<0)\n",
    "        Y[self.mask] = 0\n",
    "        return Y\n",
    "    def backward(self,dY) :\n",
    "        dX = np.ones_like(dY)\n",
    "        dX[self.mask] = 0.\n",
    "        return dX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SigmoidLayer :\n",
    "    def forward(self,X) :\n",
    "        Y = 1./(1.+np.exp(-X))\n",
    "        self.Y = Y\n",
    "        return Y\n",
    "    def backward(self,dY) :\n",
    "        Y = self.Y\n",
    "        return dY*Y*(1-Y)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftMax : \n",
    "    def forward(self,X) :\n",
    "        c= np.max(X)\n",
    "        exp_X = np.exp(X-c)\n",
    "        sum_exp_X = np.sum(exp_X)\n",
    "        Y = exp_X/sum_exp_X\n",
    "        self.Y = Y\n",
    "        return Y\n",
    "    def backward(self,dY) :\n",
    "        T = -dY*self.Y\n",
    "        return self.Y-T\n",
    "        \n",
    "    def backward(self,dY,T) :\n",
    "        return self.Y-T\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Function Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryEntropy :\n",
    "    def forward(self,T,X) :\n",
    "        self.T = T; self.X = X\n",
    "        Y = -T*np.log(eps+X) - (1.-T)*np.log(eps+1.-X)\n",
    "        return Y\n",
    "    def backward(self,dY) :\n",
    "        T = self.T\n",
    "        X = self.X\n",
    "        return -dY*(T/(X+eps) + (1.-T)/(eps+1.-X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossEntropy :\n",
    "    def __init__(self,T) :\n",
    "        self.T = T  \n",
    "    def forward(self,X) :\n",
    "        self.X = X\n",
    "        Y = - np.sum(self.T*np.log(eps+X),axis=1)\n",
    "        return Y\n",
    "    def backward(self,dY) :\n",
    "        T = self.T; X = self.X\n",
    "        return - dY*(T/X)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_Xavier_weights(n,m) :\n",
    "    W = np.sqrt(1./(n+m))*np.random.randn(n,m)\n",
    "    b = np.zeros(m)\n",
    "    return W,b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear :\n",
    "    def __init__(self,W,b) :\n",
    "        self.Mul = Mul(W)\n",
    "        self.Add = Add(b)\n",
    "    def forward(self,X) :\n",
    "        Z = self.Mul.forward(X)\n",
    "        Y = self.Add.forward(Z)\n",
    "        return Y\n",
    "        \n",
    "    def backward(self,dY) :\n",
    "        dZ,B = self.Add.backward(dY)\n",
    "        db = np.sum(B,axis=0)\n",
    "        dX,dW = self.Mul.backward(dZ)\n",
    "        return dX, dW, db\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossEntropyWithSoftMax :\n",
    "    def __init__(self,T) :\n",
    "        self.SoftMax = SoftMax()\n",
    "        self.CrossEntropy = CrossEntropy(T)\n",
    "\n",
    "    def forward(self,X) :\n",
    "        Z = self.SoftMax.forward(X)\n",
    "        Y = self.CrossEntropy.forward(Z)\n",
    "        return Y\n",
    "    def backward(self,dY) :\n",
    "        dZ = self.CrossEntropy.backward(dY)\n",
    "        dX = self.SoftMax.backward(dY,self.CrossEntropy.T)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MutiClassClassification :\n",
    "    def __init__(self,inputsize,outputsize) :\n",
    "        W1, b1 = init_Xavier_weights(inputsize,256)\n",
    "        self.Linear1 = Linear(W1,b1)\n",
    "        self.ReLU = ReLU()\n",
    "        W2,b2 = init_Xavier_weights(256,outputsize)\n",
    "        self.Linear2 = Linear(W2,b2)\n",
    "        self.Softmax = SoftMax()\n",
    "    \n",
    "    def forward(self,X) :\n",
    "        x1 = self.Linear1(X)\n",
    "        x2 = self.ReLU(x2)\n",
    "        x3 = self.Linear2(x2)\n",
    "        self.x3 = x3\n",
    "        Y = self.Softmax(x3)\n",
    "        return Y\n",
    "    \n",
    "    def backward(self,dY) :\n",
    "        dX2,dW2,db2 = self.Linear2.backward(dY)\n",
    "        dZ1 = self.ReLU.backward(dX2)\n",
    "        dX1, dW1, db1 = self.Linear1.backward(dZ1)\n",
    "        return dX1,dW1,db1,dZ1,dX2,dW2,db2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('DeepLearning')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "627c7ca4540c130a5ae87fdd551b66ef794b282493d3a91f94441822ce8cc555"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
